/** * University of Illinois/NCSA * Open Source License *  * Copyright (c) 2008, Board of Trustees-University of Illinois.   * All rights reserved. *  * Developed by:  *  * Automated Learning Group * National Center for Supercomputing Applications * http://www.seasr.org *  *   * Permission is hereby granted, free of charge, to any person obtaining a copy * of this software and associated documentation files (the "Software"), to * deal with the Software without restriction, including without limitation the * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or * sell copies of the Software, and to permit persons to whom the Software is * furnished to do so, subject to the following conditions:  *  *  * Redistributions of source code must retain the above copyright notice, *    this list of conditions and the following disclaimers.  *  *  * Redistributions in binary form must reproduce the above copyright notice, *    this list of conditions and the following disclaimers in the  *    documentation and/or other materials provided with the distribution.  *  *  * Neither the names of Automated Learning Group, The National Center for *    Supercomputing Applications, or University of Illinois, nor the names of *    its contributors may be used to endorse or promote products derived from *    this Software without specific prior written permission.  *  * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS * WITH THE SOFTWARE. */package org.seasr.components.text.transform;// ==============// Java Imports// ==============import java.util.*;import java.io.*;import java.util.logging.*;// ===============// Other Imports// ===============import org.meandre.core.*;import org.meandre.annotations.*;import org.seasr.components.text.datatype.corpora.Annotation;import org.seasr.components.text.datatype.corpora.AnnotationConstants;import org.seasr.components.text.datatype.corpora.AnnotationSet;import org.seasr.components.text.datatype.corpora.Document;import org.seasr.components.text.datatype.corpora.FeatureMap;import org.seasr.components.text.datatype.termlist.TermListLite;// import org.meandre.tools.components.*;// import org.meandre.tools.components.FlowBuilderAPI.WorkingFlow;/** * Overview: This module takes in a <i>Document</i> object that has been * tokenized and outputs a list of the tokens, or terms, and the number of times * each token appears (frequency count) as a <i>TermList</i> object. *  *  * Data Type Restrictions: The input document must have been tokenized. *  *  * Data Handling: This module creates a new TermList object for each document. *  *  * Scalability: This module makes one pass over the token list resulting in * linear time complexity per the number of tokens. Memory usage is proportional * to the number tokens. *  *  * Trigger Criteria: Standard. *  *  * @author D. Searsmith *  * TODO: Testing, Unit Testing *  */@Component(creator = "Duane Searsmith",description = "<p>Overview: <br>"		+ "This module takes in a Document object that has been tokenized and "		+ "outputs a list of the tokens, or terms, and the number of times each "		+ "token appears (frequency count) as a TermList object. </p>"		+ "<p>Data Type Restrictions: <br>"		+ "The input document must have been tokenized.</p>"		+ "<p>Data Handling: <br>"		+ "This module creates a new TermList object for each document. Note that each feature"		+ "from the Document is added to TermList properties unless the value is not of type String.</p>"		+ "<p>Scalability: <br>"		+ "This module makes one pass over the token list resulting in linear time complexity "		+ "per the number of tokens.  Memory usage is proportional to the number tokens.</p>"		+ "<p>Trigger Criteria: <br>" + "Standard.</p>",name = "DocToTermList", tags = "text document termlist transform")public class DocToTermList implements ExecutableComponent {	// ==============	// Data Members	// ==============	private int m_docsProcessed = 0;	private static Logger _logger = Logger.getLogger("DocToTermList");	// props	@ComponentProperty(description = "Verbose output? A boolean value (true or false).", name = "verbose", defaultValue = "false")	final static String DATA_PROPERTY_VERBOSE = "verbose";	@ComponentProperty(description = "Debug? A boolean value (true or false).", name = "debug", defaultValue = "false")	final static String DATA_PROPERTY_DEBUG = "debug";	@ComponentProperty(description = "Clear the document object? A boolean value (true or false).", name = "free_doc", defaultValue = "true")	final static String DATA_PROPERTY_FREE_DOC = "free_doc";	@ComponentProperty(description = "Maximum number of tokens.", name = "max_tokens", defaultValue = "-1")	final static String DATA_PROPERTY_TOKEN_LIMIT = "max_tokens";	@ComponentProperty(description = "Title weight.", name = "title_weight", defaultValue = "0")	final static String DATA_PROPERTY_TITLE_WEIGHT = "title_weight";	// io	@ComponentInput(description = "Document object.", name = "document")	public final static String DATA_INPUT_DOCUMENT = "document";	@ComponentOutput(description = "Term list object.", name = "termlist")	public final static String DATA_OUTPUT_TERMLIST = "termlist";	// ================	// Static Methods	// ================	/**	 * Test	 */	static public void main(String[] args) {		// // get a flow builder instance		// FlowBuilderAPI flowBuilder = new FlowBuilderAPI();		// // get a flow object		// WorkingFlow wflow = flowBuilder.newWorkingFlow("test");		// // add a component		// String pushString = wflow		// .addComponent("org.seasr.meandre.components.io.PushString");		// // set a component property		// wflow.setComponentInstanceProp(pushString, "string",		// "http://norma.ncsa.uiuc.edu/public-dav/capitanu");		// // add another component		// String reader = wflow		// .addComponent("org.seasr.meandre.components.t2k.io.file.ReadFileNames");		// wflow.setComponentInstanceProp(reader, DATA_PROPERTY_FILTER,		// ".*\\.xml");		// wflow.setComponentInstanceProp(reader, DATA_PROPERTY_WEBDAV, "true");		// // make a connection between two components		// wflow.connectComponents(pushString, "output_string", reader,		// DATA_INPUT_DIRNAME);		//		// // execute the flow specifying that we want a web UI displayed		// flowBuilder.execute(wflow, false);		//		// // For some reason the process does not end without a forced exit.		// System.exit(0);	}	// ================	// Constructor(s)	// ================	public DocToTermList() {	}	// ================	// Public Methods	// ================	public boolean getVerbose(ComponentContextProperties ccp) {		String s = ccp.getProperty(DATA_PROPERTY_VERBOSE);		return Boolean.parseBoolean(s.toLowerCase());	}	public boolean getDebug(ComponentContextProperties ccp) {		String s = ccp.getProperty(DATA_PROPERTY_DEBUG);		return Boolean.parseBoolean(s.toLowerCase());	}	public boolean getFreeDoc(ComponentContextProperties ccp) {		String s = ccp.getProperty(DATA_PROPERTY_FREE_DOC);		return Boolean.parseBoolean(s.toLowerCase());	}	public int getTokenLimit(ComponentContextProperties ccp) {		String s = ccp.getProperty(DATA_PROPERTY_TOKEN_LIMIT);		return Integer.parseInt(s);	}	public int getTitleWeight(ComponentContextProperties ccp) {		String s = ccp.getProperty(DATA_PROPERTY_TITLE_WEIGHT);		return Integer.parseInt(s);	}	// =====================================	// Interface Impl: ExecutableComponent	// =====================================	public void initialize(ComponentContextProperties ccp) {		_logger.fine("initialize() called");		m_docsProcessed = 0;	}	public void dispose(ComponentContextProperties ccp) {		_logger.fine("dispose() called");		if (getVerbose(ccp) || getDebug(ccp)) {			_logger.info("\nEND EXEC -- DocumentToTermList -- Docs Processed: "					+ m_docsProcessed + "\n");		}		m_docsProcessed = 0;	}	/**	 * In frequency include all occurrences of a term even if it only matches	 * the POS tag criteria for a subset of occurrences.	 */	public void execute(ComponentContext ctx)			throws ComponentExecutionException, ComponentContextException {		_logger.fine("execute() called");		//props ==============		boolean verbose = this.getVerbose(ctx);		int toklim = this.getTokenLimit(ctx);		boolean debug = this.getDebug(ctx);		int titwt = this.getTitleWeight(ctx);		boolean freedoc = this.getFreeDoc(ctx);		//====================		int tokens_processed = 0;		try {			Document doc = (Document) ctx					.getDataComponentFromInput(DATA_INPUT_DOCUMENT);			if (debug) {				_logger.info("Document: "						+ doc.getDocID()						+ " has "						+ doc.getAnnotations().get(								AnnotationConstants.TOKEN_ANNOT_TYPE).size()						+ " tokens.");			}			AnnotationSet annots = doc.getAnnotations(AnnotationConstants.ANNOTATION_SET_TOKENS);			Map<String, TLCont> map = new HashMap<String, TLCont>();			for (Iterator<Annotation> iter = annots.iterator(); iter.hasNext();) {				Annotation tok = iter.next();				if (tok.getType().equals(AnnotationConstants.TOKEN_ANNOT_TYPE)) {					String tokimg = (String) tok.getFeatures().get(							AnnotationConstants.TOKEN_ANNOT_FEAT_NORM_IMAGE);					String otokimg = tok.getContent(doc);					if (tokimg == null) {						tokimg = otokimg;					}					if (tokimg.length() > 0) {						TLCont obarr = map.get(tokimg);						if (obarr == null) {							obarr = new TLCont();							obarr._img = tokimg;							obarr._cnt = 1;							HashSet<String> set = new HashSet<String>();							set.add(otokimg);							obarr._orig_imgs = set;							if (tok									.getFeatures()									.containsKey(											AnnotationConstants.TOKEN_ANNOT_FEAT_OCCURENCE_MULTIPLIER)) {								obarr._occurence_multiplier = Integer										.parseInt(tok												.getFeatures()												.get(														AnnotationConstants.TOKEN_ANNOT_FEAT_OCCURENCE_MULTIPLIER));							}							map.put(tokimg, obarr);						} else {							obarr._cnt++;							obarr._orig_imgs.add(otokimg);						}						String intit = tok								.getFeatures()								.get(										AnnotationConstants.TOKEN_ANNOT_FEAT_INTITLE_BOOL);						if (intit != null) {							obarr._inTitle = Boolean.valueOf(intit);						}						tokens_processed++;					}				}			}			// for ngrams			for (Iterator<Annotation> iter = annots.iterator(); iter.hasNext();) {				Annotation tok = iter.next();				if (tok.getType().equals(AnnotationConstants.NGRAM_ANNOT_TYPE)) {					String tokimg = (String) tok.getFeatures().get(							AnnotationConstants.NGRAM_ANNOT_FEAT_NORM_IMAGE);					String otokimg = tok.getContent(doc);					if (tokimg == null) {						tokimg = otokimg;					}					if (tokimg.length() > 0) {						TLCont obarr = map.get(tokimg);						if (obarr == null) {							obarr = new TLCont();							obarr._img = tokimg;							obarr._cnt = 1;							HashSet<String> set = new HashSet<String>();							set.add(otokimg);							obarr._orig_imgs = set;							if (tok									.getFeatures()									.containsKey(											AnnotationConstants.TOKEN_ANNOT_FEAT_OCCURENCE_MULTIPLIER)) {								obarr._occurence_multiplier = Integer										.parseInt(tok												.getFeatures()												.get(														AnnotationConstants.TOKEN_ANNOT_FEAT_OCCURENCE_MULTIPLIER));							}							map.put(tokimg, obarr);						} else {							obarr._cnt++;							obarr._orig_imgs.add(otokimg);						}						tokens_processed++;					}				}			}			// Now sort by freq descending			TreeSet<TLCont> ts = new TreeSet<TLCont>(new TokElementComparator());			ts.addAll(map.values());			// load into TermListLite			TermListLite tlist = new TermListLite();			int cnt = Integer.MAX_VALUE;			if (toklim > 0) {				cnt = toklim;			}			for (Iterator<TLCont> it = ts.iterator(); it.hasNext() && (cnt > 0); cnt--) {				TLCont obarr = it.next();				int freq = obarr._cnt;				if (obarr._inTitle) {					freq += titwt;				}				if (obarr._occurence_multiplier > 0) {					freq = freq * obarr._occurence_multiplier;				}				if (freq > 0) {					tlist.addTerm(obarr._img, freq, new ArrayList<String>(							obarr._orig_imgs));				}			}			tlist.setDocID(doc.getDocID());			tlist.setTitle(doc.getTitle());			tlist.setDate(doc.getDate());			try {				tlist.setProperties(doc.getFeatures());				if (verbose || debug)					_logger.info("DocToTermList: set properties successfully");			} catch (NullPointerException e) {				try {					FeatureMap fMap = doc.getFeatures();					Iterator<String> keys = fMap.keySet().iterator();					Map<String, String> newMap = new HashMap<String, String>();					while (keys.hasNext()) {						String key = (String) keys.next();						String value = (String) fMap.get(key);						if ((value != null) && (value instanceof String)) {							newMap.put(key, value);						} else {							if (verbose)								_logger										.info("DocToTermList: removed property key - "												+ key												+ " cause it had a null value (for document "												+ doc.getDocID() + ".");						}					}// while					tlist.setProperties(newMap);					if (verbose || debug)						_logger								.info("DocToTermList: set properties successfully for document "										+ doc.getDocID());				} catch (Exception ex) {					_logger.severe("DocToTermList: " + doc.getDocID());					ex.printStackTrace();				}			}			if (verbose || debug) {				_logger.info(tokens_processed						+ " tokens were processed for this document -- "						+ tlist.getTitle());				_logger.info("# terms in list: " + tlist.getSize() + "\n\n");			}			if (freedoc) {				doc.free();			}			ctx.pushDataComponentToOutput(DATA_OUTPUT_TERMLIST, tlist);			m_docsProcessed++;		} catch (Exception ex) {			ex.printStackTrace();			_logger.severe(ex.getMessage());			_logger.severe("ERROR: DocToTermList.execute()");			throw new ComponentExecutionException(ex);		}	}	// =============	// Inner Class	// =============	private class TLCont {		String _img = null;		int _cnt = -1;		Set<String> _orig_imgs = null;		boolean _inTitle = Boolean.FALSE;		int _occurence_multiplier = -1;	}	public class TokElementComparator implements Comparator<TLCont>,			Serializable {		private static final long serialVersionUID = 1L;		/**		 * 		 * put your documentation comment here		 * 		 */		public TokElementComparator() {		}		// ======================		// Interface: Comparator		// ======================		public int compare(TLCont o1, TLCont o2) {			int pos1 = o1._cnt;			int pos2 = o2._cnt;			if (pos1 < pos2) {				return 1;			} else if (pos1 > pos2) {				return -1;			} else {				return o1._img.compareTo(o2._img);			}		}		/**		 * 		 * put your documentation comment here		 * 		 * @param o		 * 		 * @return		 * 		 */		public boolean equals(Object o) {			return this.equals(o);		}	}}